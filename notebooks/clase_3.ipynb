{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c27a603",
   "metadata": {},
   "source": [
    "## Tools y Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2f4e7",
   "metadata": {},
   "source": [
    "#### Â¿QuÃ© son las Tools (Herramientas)?\n",
    "\n",
    "Las herramientas son funciones en algÃºn lenguaje de programaciÃ³n que encapsulan una **acciÃ³n externa** al LLM. Son el puente entre la capacidad de razonamiento del LLM y el mundo real (APIs, bases de datos, sistemas de archivos).\n",
    "\n",
    "  * **PropÃ³sito:** Proveer al LLM de **informaciÃ³n externa** a su base de entrenamiento (como datos de la web, cÃ¡lculos en tiempo real, o cÃ³digo).\n",
    "  * **Esquema:** Las herramientas exponen su lÃ³gica y su **esquema de datos de entrada** (`a: int, b: int`) para que el LLM pueda decidir quÃ© necesita y con quÃ© argumentos debe llamarla."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550dd69",
   "metadata": {},
   "source": [
    "####  DefiniciÃ³n de Herramientas Personalizadas\n",
    "\n",
    "Podemos definir cualquier funciÃ³n de Python como una herramienta usando el decorador `@tool` de LangChain. Este decorador automÃ¡ticamente convierte la funciÃ³n, sus argumentos y su *docstring* en el esquema de datos que el LLM puede leer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "\n",
    "# AsegÃºrate de configurar tu clave API de Groq en las variables de entorno\n",
    "api_key=os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "# 1. Herramienta Personalizada: Sumar dos nÃºmeros\n",
    "@tool\n",
    "def sumar_numeros(a: int, b: int) -> int:\n",
    "    \"\"\"Suma dos nÃºmeros enteros y devuelve el resultado.\"\"\"\n",
    "    print(f\"\\nâš™ï¸ [TOOL]: Ejecutando suma de {a} + {b}\")\n",
    "    return a + b\n",
    "\n",
    "# 2. Inicializar el LLM y bindear (atar) la herramienta\n",
    "# Usamos un modelo optimizado y le 'enseÃ±amos' la herramienta.\n",
    "try:\n",
    "    llm_with_tools = ChatGroq(model=\"llama-3.3-70b-versatile\", api_key=api_key, temperature=0).bind_tools([sumar_numeros])\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ERROR al inicializar ChatGroq. AsegÃºrate de tener la clave API configurada. Usando simulaciÃ³n de LLM.\")\n",
    "    llm_with_tools = None\n",
    "\n",
    "pregunta_usuario = \"Â¿CuÃ¡l es la suma de 125 y 48?\"\n",
    "messages = [HumanMessage(content=pregunta_usuario)]\n",
    "\n",
    "print(f\"--- FASE 1: LLM recibe pregunta y decide la acciÃ³n ---\")\n",
    "print(f\"Pregunta: {pregunta_usuario}\\n\")\n",
    "\n",
    "if llm_with_tools:\n",
    "    # 1. El LLM genera la PeticiÃ³n de Tool Call\n",
    "    response_1 = llm_with_tools.invoke(messages)\n",
    "else:\n",
    "    # SimulaciÃ³n de la respuesta si falla la API\n",
    "    from langchain_core.messages import ToolCall\n",
    "    response_1 = AIMessage(\n",
    "        content=\"\",\n",
    "        tool_calls=[ToolCall(name=\"sumar_numeros\", args={\"a\": 125, \"b\": 48}, id=\"call_123\")]\n",
    "    )\n",
    "\n",
    "if response_1.tool_calls:\n",
    "    llamada = response_1.tool_calls[0]\n",
    "    \n",
    "    print(\"ðŸ¤– Respuesta del LLM (AIMessage):\")\n",
    "    print(f\"  - Contenido: {response_1.content} (VacÃ­o, porque priorizÃ³ la herramienta)\")\n",
    "    print(f\"  - Herramienta solicitada: {llamada.get('name')}, Args: {llamada.get('args')}\")\n",
    "\n",
    "    # --- FASE 2: EjecuciÃ³n de la Herramienta ---\n",
    "    \n",
    "    # 2. Ejecutar la funciÃ³n solicitada\n",
    "    tool_to_call = sumar_numeros\n",
    "    tool_output = sumar_numeros.invoke(llamada.get(\"args\"))\n",
    "    \n",
    "    print(f\"\\nâš™ï¸ Resultado de la Herramienta: {tool_output}\")\n",
    "\n",
    "    # --- FASE 3: Devolver el Resultado al LLM para la Respuesta Final ---\n",
    "    \n",
    "    # 3. Crear el mensaje de retroalimentaciÃ³n (ToolMessage)\n",
    "    # Enviamos la respuesta original del LLM (response_1) + el resultado de la herramienta.\n",
    "    messages.append(response_1) \n",
    "    messages.append(ToolMessage(\n",
    "        content=str(tool_output),\n",
    "        tool_call_id=llamada.get(\"id\")\n",
    "    ))\n",
    "    \n",
    "    print(\"\\n--- FASE 4: LLM recibe el resultado y genera la respuesta final ---\")\n",
    "    \n",
    "    # 4. Invocar el LLM de nuevo con todo el historial\n",
    "    if llm_with_tools:\n",
    "        final_response = llm_with_tools.invoke(messages)\n",
    "    else:\n",
    "        final_response = AIMessage(content=f\"La suma de 125 y 48 es 173.\")\n",
    "\n",
    "    print(\"âœ… Respuesta Final del LLM:\")\n",
    "    print(final_response.content)\n",
    "\n",
    "else:\n",
    "    print(\"El LLM respondiÃ³ directamente. Esto ocurre si la pregunta es simple o si no logra mapear a una herramienta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af076f1",
   "metadata": {},
   "source": [
    "### Ejemplo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea99c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56e2f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99bf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "llm = ChatGroq(\n",
    "\tmodel=\"llama-3.3-70b-versatile\",\n",
    "\ttemperature=0.0,\n",
    "\tapi_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "llm = llm.bind_tools(tools)\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an kindly agent which goal is to resolve the user query using the tools that you have access to.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "\t(\"system\", SYSTEM_PROMPT),\n",
    "\tMessagesPlaceholder(variable_name=\"messages\")\n",
    "\t],\n",
    "\ttemplate_format=\"jinja2\"\n",
    ")\n",
    "chain = prompt | llm\n",
    "\n",
    "def llm_call(state: MessagesState):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            chain.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ],\n",
    "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2ead6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import  END\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # Si el LLM realiza un llamado a una herramienta\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "\n",
    "    # Si no, responder al usuario\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836af3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "agent_builder.add_node(\"llm\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "agent_builder.add_edge(START, \"llm\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm\")\n",
    "\n",
    "agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81b15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f83865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d85829",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Add 3 and 4, then multiply by 9 en divide by 3.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
