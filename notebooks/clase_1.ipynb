{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1550ffa3",
   "metadata": {},
   "source": [
    "## Fundamentos de LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd278c7d",
   "metadata": {},
   "source": [
    "### LLM: Una Máquina de Probabilidad de Alto Nivel\n",
    "\n",
    "Un **LLM** es un modelo que ha aprendido las **reglas estadísticas** de cómo se relacionan las palabras. Su única función es la **predicción secuencial del *token* más probable** que sigue a una entrada dada.\n",
    "\n",
    "- **Entrenamiento:** El modelo aprende de vastas cantidades de texto prediciendo la siguiente palabra. Esto le confiere una comprensión profunda de la gramática, el estilo y las relaciones conceptuales.\n",
    "- **La Salida es Probabilidad:** Cuando introduces un *prompt*, el LLM genera una **distribución de probabilidad** sobre todos los posibles *tokens* para el siguiente paso. El texto final es la selección óptima de estos *tokens* a lo largo de muchos pasos.\n",
    "- Utilizamos la arquitectura **Transformer** porque su mecanismo de **Atención (*Attention*)** permite que el modelo, al generar cada *token*, pese y le asigne importancia a **todos los *tokens* anteriores** en el *prompt*. Esto le permite manejar dependencias complejas y de larga distancia, capturando el verdadero **contexto**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968227d6",
   "metadata": {},
   "source": [
    "### Tokens: La Unidad de Computación\n",
    "\n",
    "- **¿Qué son?** Los *tokens* son las unidades mínimas de texto que el modelo procesa. No siempre son palabras completas; a menudo son prefijos, sufijos o partes de palabras. Por ejemplo, \"desconcertante\" se puede dividir en: des, concer, tante.\n",
    "- **¿Por qué Tokens?** Para manejar un **vocabulario más pequeño y eficiente**. Usar palabras completas haría el vocabulario del modelo gigantesco. Los *tokens* permiten representar virtualmente cualquier palabra, incluida jerga o palabras nuevas, con un conjunto finito y manejable de unidades."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8988261",
   "metadata": {},
   "source": [
    "### Embeddings: El Mapa Semántico\n",
    "\n",
    "- **¿Qué son?** Son **vectores numéricos** (listas de números) que representan el **significado semántico** de una pieza de texto (un *token*, una palabra, una frase).\n",
    "- Si dos conceptos son similares (ej., \"perro\" y \"cachorro\"), sus vectores de *embedding* estarán **cerca** en un espacio multidimensional. El LLM opera en este espacio matemático de significado.\n",
    "- **Uso Práctico (Búsqueda):** Los *embeddings* permiten realizar **búsquedas semánticas**. En lugar de buscar la palabra exacta (\"base de datos\"), se busca el significado, encontrando resultados como \"almacenamiento de información\" o \"repositorio\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b6e5a",
   "metadata": {},
   "source": [
    "### La Limitación Fundamental: La Ventana de Contexto\n",
    "\n",
    "#### A. El \"Por Qué\" de la Limitación (Costo Computacional)\n",
    "\n",
    "La Ventana de Contexto (el máximo de *tokens* que el LLM puede ver y generar) existe debido al **costo computacional** del mecanismo de Atención del Transformer.\n",
    "\n",
    "- **Crecimiento Cuadrático (O(n2)):** El tiempo y la memoria necesarios para procesar la atención crecen de forma **cuadrática** con la longitud del *prompt* (n). Duplicar el *prompt* cuadriplica la exigencia de recursos.\n",
    "- **Implicación:** Esto impone un límite físico (ej., 128k *tokens*) que el modelo no puede superar en una sola llamada.\n",
    "\n",
    "#### B. La Consecuencia: LLMs son Stateless\n",
    "\n",
    "Debido a esta limitación, el LLM es inherentemente **sin estado (*stateless*)**. Cada llamada a la API es tratada como una nueva interacción, **olvidando** todo lo que se dijo en llamadas anteriores, a menos que se le **vuelva a pasar** el historial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87a6541",
   "metadata": {},
   "source": [
    "### Ejemplo: llamar a un LLM usando Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff0e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism in transformers is a key component that allows the model to focus on specific parts of the input sequence when generating output. It's a powerful tool for handling sequential data, such as text, images, or audio, and has become a standard component in many state-of-the-art models.\n",
      "\n",
      "**What is attention?**\n",
      "\n",
      "Attention is a technique that enables the model to allocate its \"attention\" (or weights) to different parts of the input sequence, depending on their relevance to the task at hand. This is different from traditional recurrent neural networks (RNNs), which process the input sequence one step at a time, using the previous steps to inform the next one.\n",
      "\n",
      "**How does attention work in transformers?**\n",
      "\n",
      "In transformers, attention is implemented through a mechanism called self-attention. Self-attention allows the model to attend to all positions in the input sequence simultaneously and weigh their importance. The self-attention mechanism consists of three main components:\n",
      "\n",
      "1. **Query (Q)**: The query represents the context in which the attention is being applied. It's typically the output of the previous layer or the input sequence itself.\n",
      "2. **Key (K)**: The key represents the information that the model is trying to attend to. It's also derived from the input sequence.\n",
      "3. **Value (V)**: The value represents the information that the model wants to retrieve from the input sequence.\n",
      "\n",
      "The self-attention mechanism computes the attention weights by taking the dot product of the query and key vectors, divided by the square root of the key's dimensionality. The attention weights are then used to compute a weighted sum of the value vectors.\n",
      "\n",
      "**Mathematical formulation**\n",
      "\n",
      "The self-attention mechanism can be formulated mathematically as:\n",
      "\n",
      "`Attention(Q, K, V) = softmax(Q * K^T / sqrt(d)) * V`\n",
      "\n",
      "where `Q`, `K`, and `V` are the query, key, and value vectors, respectively, `d` is the dimensionality of the key vector, and `softmax` is the softmax activation function.\n",
      "\n",
      "**Multi-head attention**\n",
      "\n",
      "Transformers use a technique called multi-head attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This is achieved by:\n",
      "\n",
      "1. Splitting the input sequence into multiple attention heads.\n",
      "2. Applying self-attention to each head independently.\n",
      "3. Concatenating the outputs from each head.\n",
      "4. Linearly transforming the concatenated output.\n",
      "\n",
      "Multi-head attention allows the model to capture a wider range of contextual relationships in the input sequence, making it more effective for tasks like language translation, question answering, and text classification.\n",
      "\n",
      "**Benefits of attention**\n",
      "\n",
      "The attention mechanism in transformers provides several benefits, including:\n",
      "\n",
      "* **Parallelization**: Attention allows the model to process the input sequence in parallel, making it more efficient than RNNs.\n",
      "* **Flexibility**: Attention enables the model to attend to different parts of the input sequence, depending on the task requirements.\n",
      "* **Improved performance**: Attention has been shown to improve the performance of transformers on a wide range of tasks, including language translation, question answering, and text classification.\n",
      "\n",
      "In summary, the attention mechanism in transformers is a powerful tool that enables the model to focus on specific parts of the input sequence, depending on their relevance to the task at hand. By using self-attention, multi-head attention, and parallelization, transformers can capture a wide range of contextual relationships in the input sequence, making them highly effective for many NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the attention mechanism in transformers\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75891dd",
   "metadata": {},
   "source": [
    "## Máquina de Estado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea9d1b9",
   "metadata": {},
   "source": [
    "###  ¿Qué es una Máquina de Estado?\n",
    "\n",
    "Una **Máquina de Estado Finito (MEF)** es un modelo que describe el comportamiento de un sistema que solo puede estar en un **número limitado de situaciones** o \"modos de operación\" llamados **estados**.\n",
    "\n",
    "Ejemplo: un semáforo. El semáforo no puede estar en Rojo y en Verde al mismo tiempo; solo puede estar en uno de esos estados en cualquier instante temporal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081b69f",
   "metadata": {},
   "source": [
    "Una máquina de estados tiene tres elementos esenciales:\n",
    "\n",
    "- **Estados (States):** Las condiciones o modos en los que puede encontrarse el sistema (Ej: Verde, Amarillo o Rojo).\n",
    "- **Eventos (Inputs):** Algo que sucede (una entrada, una señal, un tiempo transcurrido) que impulsa el cambio de estado (Ej: Transcurrieron 120 segundos).\n",
    "- **Transiciones (Transitions):** Las reglas que definen el paso de un estado a otro en respuesta a un evento (Ej: Si estás en Verde y ocurre el evento Transcurrieron 120 segundos, cambias al estado Amarillo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcabab",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f506f9c8",
   "metadata": {},
   "source": [
    "LangGraph es un framework de orquestación de bajo nivel para construir, gestionar y desplegar agentes (y workflows) con estado y de larga ejecución.\n",
    "\n",
    "¿Qué lo diferencia de otros frameworks como CrewAI, AutoGen, LlamaIndex, etc? LangGraph, como su nombre indica, está basado en grafos y podemos verlo también como una máquina de estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72e134a",
   "metadata": {},
   "source": [
    "En su núcleo, los flujos y agentes en LangGraph funcionan como grafos. Nosotros definimos su comportamiento usando tres componentes principales:\n",
    "\n",
    "- **State** (Estado): Estructura de dato **compartida** que representa la snapshot actual de la aplicación. Puede ser cualquier tipo de dato.\n",
    "- **Nodes** (Nodos): Funciones que codifican la lógica de los agentes o los flujos. Reciben el estado actual como entrada, realizan alguna operación computacional o alguna modificación y retorna un estado actualizado.\n",
    "- **Edges** (Aristas): Funciones que determinan cuál nodo ejecutar a partir del estado actual. Pueden ser ramas condicionales o transiciones fijas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bd78b",
   "metadata": {},
   "source": [
    "![](https://img.notionusercontent.com/s3/prod-files-secure%2Fb546fe1d-7c90-4ed3-9bfc-b9277dba7d5f%2Fc832fade-a1ca-487f-a5f1-d628a3584a22%2Fimage.png/size/w=2000?exp=1760491798&sig=-zpx9bKTcpRJOX6ouIw3wou5hUeA4sg-_T6j-l1G-iA&id=284f806c-2f03-80fa-a836-f3b542f112f7&table=block&userId=2206c248-620a-47ed-be6d-56e04275be59)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d53c30",
   "metadata": {},
   "source": [
    "Utilizando nodos y aristas podemos crear flujos complejos y repetitivos que van evolucionando el estado en el tiempo.\n",
    "\n",
    "En resumen: los nodos hacen el trabajo, las aristas dicen qué hacer a continuación.\n",
    "\n",
    "El verdadero poder de LangGraph viene de cómo maneja el estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cfedaa",
   "metadata": {},
   "source": [
    "¿Por qué LangGraph es una máquina de estado?\n",
    "\n",
    "Tenemos el **State:** Almacena el **contexto compartido** (p. ej., el historial de la conversación, los datos recopilados, el resultado del LLM) que es visible para todos los agentes/nodos.\n",
    "\n",
    "Los **nodos:** Son las unidades de trabajo que modifican el estado. Cada nodo representa una **tarea de IA** o lógica (p. ej., \"Generar Respuesta con LLM\", \"Buscar Datos en DB\", \"Decidir Siguiente Paso\").\n",
    "Y las **aristas:** Son la **lógica de control de flujo**. Determinan dinámicamente qué agente o tarea debe ejecutarse a continuación, basándose en el contenido del **`State`** actual.\n",
    "\n",
    "Que llevándolo a los términos de una máquina de estado, son, respectivamente, los estados, las acciones y las transiciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf5d51",
   "metadata": {},
   "source": [
    "En esencia: LangGraph permite construir un sistema donde un LLM o un fragmento de código (el Node) lee el State, realiza una acción, actualiza el State, y luego una función de enrutamiento (Edge) lee ese nuevo State para decidir a qué otro nodo debe pasar la ejecución. Esto crea un ciclo iterativo y dinámico que es la base de los agentes de IA complejos y las conversaciones multi-turno."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101b7e8",
   "metadata": {},
   "source": [
    "Finalmente, el último concepto fundamental para comprender de LangGraph es que es **Stateful (con estado)** porque **recuerda y utiliza información de sus interacciones o pasos anteriores** para determinar su comportamiento actual y futuro.\n",
    "\n",
    "Se puede interpretar \"stateful\" así:\n",
    "\n",
    "1. **Memoria de Corto Plazo:** El sistema no trata cada paso o interacción como algo nuevo. El componente **`State`** de LangGraph actúa como la memoria central compartida.\n",
    "2. **Impacto Secuencial:** El resultado de un nodo (una acción) se guarda en el **`State`** y afecta directamente al siguiente nodo y a las transiciones (`Edges`). Por ejemplo, si un nodo decide que la respuesta del LLM es insuficiente, esa decisión se guarda en el estado, lo que lleva a un nodo de \"Investigación\" en lugar de un nodo de \"Respuesta Final\".\n",
    "3. **Flujo de Larga Duración:** Permite crear agentes que tienen un **\"ciclo de vida\"** complejo (long-running). Un agente puede pasar por varios nodos, volver a un nodo anterior (bucle), o pausarse y reanudarse más tarde, todo gracias a que el estado se mantiene y gestiona de forma coherente.\n",
    "\n",
    "Un sistema **stateful** puede responder a la pregunta: *\"Dado lo que acabo de hacer y la información que tengo ahora, ¿cuál es mi próximo paso?\"*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
